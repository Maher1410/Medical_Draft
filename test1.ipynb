{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5b1273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#%% -------- 1. Configuration & Imports --------\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import splitfolders\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torchmetrics\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Configuration\n",
    "NUM_CLASSES = 4\n",
    "IMG_SIZE = 384\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "PRECISION = '16-mixed'\n",
    "EPOCHS = 150\n",
    "LR = 2e-4\n",
    "WARMUP_PCT = 0.1\n",
    "DATA_ROOT = \"C:\\\\Users\\\\DELL 5540\\\\Desktop\\\\Brachial\\\\2nd Classification\"\n",
    "SPLIT_ROOT = \"C:\\\\Users\\\\DELL 5540\\\\Desktop\\\\Brachial\\\\Split_Dataset\"\n",
    "SEED = 42\n",
    "CLASS_NAMES = ['Type 0', 'Type 1 Neurapraxia', 'Type 2 Axonotemsis', 'Type 3 Neurotmesis']\n",
    "\n",
    "# Reproducibility\n",
    "pl.seed_everything(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435fa390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 2. Medical Data Pipeline --------\n",
    "class MedicalAugmentation:\n",
    "    \"\"\"MRI-optimized augmentation pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        self.affine = transforms.RandomAffine(\n",
    "            degrees=(-7, 7), \n",
    "            translate=(0.05, 0.05),\n",
    "            scale=(0.9, 1.1)\n",
    "        )\n",
    "        self.elastic = transforms.ElasticTransform(\n",
    "            alpha=50.0,\n",
    "            sigma=5.0\n",
    "        )\n",
    "        self.color_jitter = transforms.ColorJitter(\n",
    "            brightness=0.1,\n",
    "            contrast=0.2,\n",
    "            saturation=0.1\n",
    "        )\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        img = self.affine(img)\n",
    "        if np.random.rand() > 0.7:\n",
    "            img = self.elastic(img)\n",
    "        img = self.color_jitter(img)\n",
    "        return img\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    MedicalAugmentation(),\n",
    "    transforms.RandomErasing(p=0.4, scale=(0.02, 0.15)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset preparation\n",
    "if not os.path.exists(os.path.join(SPLIT_ROOT, 'train')):\n",
    "    splitfolders.ratio(\n",
    "        DATA_ROOT,\n",
    "        output=SPLIT_ROOT,\n",
    "        seed=SEED,\n",
    "        ratio=(0.8, 0.1, 0.1),\n",
    "        group_prefix=None,\n",
    "        move=False,\n",
    "        oversample='auto'\n",
    "    )\n",
    "\n",
    "train_dataset = ImageFolder(os.path.join(SPLIT_ROOT, 'train'), train_transform)\n",
    "val_dataset = ImageFolder(os.path.join(SPLIT_ROOT, 'val'), val_transform)\n",
    "test_dataset = ImageFolder(os.path.join(SPLIT_ROOT, 'test'), val_transform)\n",
    "\n",
    "# Class balancing\n",
    "class_counts = np.bincount(train_dataset.targets)\n",
    "class_weights = torch.tensor([1.0 / (count + 1e-5) for count in class_counts], dtype=torch.float32)\n",
    "sampler = WeightedRandomSampler(weights=class_weights[train_dataset.targets], num_samples=len(train_dataset), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb8415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 3. Enhanced ResMT Architecture --------\n",
    "class MedicalResMT(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Hybrid Backbone\n",
    "        self.cnn = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-3])  # Until layer3\n",
    "        \n",
    "        # 2. Feature Adaptation\n",
    "        self.adapt = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool2d((14, 14)))\n",
    "        \n",
    "        # 3. Positional Encoding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 196, 512) * 0.02)\n",
    "        \n",
    "        # 4. Transformer Encoder\n",
    "        self.transformer = nn.Sequential(\n",
    "            TransformerEncoderLayer(\n",
    "                d_model=512,\n",
    "                nhead=8,\n",
    "                dim_feedforward=2048,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            TransformerEncoderLayer(\n",
    "                d_model=512,\n",
    "                nhead=8,\n",
    "                dim_feedforward=2048,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # 5. Attention Gate\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 6. Classifier Head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes))\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Transformer initialization\n",
    "        for m in self.transformer.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                    \n",
    "        # Attention initialization\n",
    "        nn.init.kaiming_normal_(self.attention[0].weight)\n",
    "        nn.init.kaiming_normal_(self.attention[2].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN Features\n",
    "        x = self.cnn(x)  # [B, 1024, 28, 28]\n",
    "        x = self.adapt(x)  # [B, 512, 14, 14]\n",
    "        \n",
    "        # Prepare for Transformer\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C, -1).permute(0, 2, 1)  # [B, 196, 512]\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer Processing\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Attention-based Pooling\n",
    "        attn_weights = self.attention(x)  # [B, 196, 1]\n",
    "        x = torch.sum(x * attn_weights, dim=1)  # Weighted sum\n",
    "        \n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f06afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL 5540\\AppData\\Local\\Temp\\ipykernel_19600\\2705675882.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast()\n",
      "C:\\Users\\DELL 5540\\AppData\\Local\\Temp\\ipykernel_19600\\2705675882.py:88: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast()\n"
     ]
    }
   ],
   "source": [
    "#%% -------- 4. Optimized Training Setup --------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('alpha', alpha)  # Store as buffer\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.alpha is not None:\n",
    "            # Ensure alpha is on same device as inputs\n",
    "            alpha = self.alpha.to(inputs.device)\n",
    "        else:\n",
    "            alpha = None\n",
    "            \n",
    "        ce_loss = nn.functional.cross_entropy(\n",
    "            inputs, \n",
    "            targets, \n",
    "            reduction='none', \n",
    "            weight=alpha\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return loss.mean()\n",
    "    \n",
    "\n",
    "class LitMedicalModel(pl.LightningModule):\n",
    "    def __init__(self, total_steps):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = MedicalResMT(NUM_CLASSES)\n",
    "        \n",
    "        # Move alpha to device using register_buffer\n",
    "        self.register_buffer(\n",
    "            'class_weights',\n",
    "            torch.tensor([1.0, 2.5, 3.0, 4.0], dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        self.criterion = FocalLoss(\n",
    "            alpha=self.class_weights,  # Use registered buffer\n",
    "            gamma=2.5\n",
    "        )\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES)\n",
    "        self.val_auc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES)\n",
    "        self.val_f1 = torchmetrics.F1Score(task='multiclass', num_classes=NUM_CLASSES)\n",
    "        \n",
    "        # Training state\n",
    "        self.total_steps = total_steps\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Layer-wise optimization\n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': self.model.cnn.parameters(), 'lr': LR/10},\n",
    "            {'params': self.model.adapt.parameters(), 'lr': LR/5},\n",
    "            {'params': self.model.transformer.parameters(), 'lr': LR},\n",
    "            {'params': self.model.head.parameters(), 'lr': LR}\n",
    "        ], weight_decay=0.05)\n",
    "        \n",
    "        # OneCycle schedule\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=[LR/10, LR/5, LR, LR],\n",
    "            total_steps=self.total_steps,\n",
    "            pct_start=WARMUP_PCT,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n",
    "\n",
    "    @autocast()  \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Medical Mixup\n",
    "        x, y = batch\n",
    "        lam = np.random.beta(0.2, 0.2)\n",
    "        idx = torch.randperm(x.size(0))\n",
    "        \n",
    "        mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "        logits = self.model(mixed_x)\n",
    "        \n",
    "        loss = lam * self.criterion(logits, y) + (1 - lam) * self.criterion(logits, y[idx])\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_acc(logits.softmax(dim=1), y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    @autocast()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        self.val_auc(logits.softmax(dim=1), y)\n",
    "        self.val_f1(logits.softmax(dim=1), y)\n",
    "        self.log_dict({\n",
    "            'val_loss': loss,\n",
    "            'val_auc': self.val_auc,\n",
    "            'val_f1': self.val_f1\n",
    "        }, prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f0c7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | MedicalResMT       | 19.9 M\n",
      "1 | criterion | FocalLoss          | 0     \n",
      "2 | train_acc | MulticlassAccuracy | 0     \n",
      "3 | val_auc   | MulticlassAUROC    | 0     \n",
      "4 | val_f1    | MulticlassF1Score  | 0     \n",
      "-------------------------------------------------\n",
      "19.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "19.9 M    Total params\n",
      "79.734    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c2b4b58b2e485085f7a024582def15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m\n\u001b[0;32m     29\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     30\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[0;32m     32\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     33\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     37\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     38\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    575\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    577\u001b[0m     ckpt_path,\n\u001b[0;32m    578\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    580\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 990\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1034\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1036\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1063\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1060\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1063\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 94\u001b[0m, in \u001b[0;36mLitMedicalModel.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     91\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[0;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(logits, y)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_auc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_f1(logits\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), y)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dict({\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss,\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_auc,\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_f1\n\u001b[0;32m    100\u001b[0m }, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchmetrics\\metric.py:315\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchmetrics\\metric.py:385\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 385\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_count \u001b[38;5;241m=\u001b[39m _update_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchmetrics\\metric.py:699\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;66;03m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;66;03m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;66;03m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_context(\n\u001b[0;32m    695\u001b[0m     dist_sync_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_fn,\n\u001b[0;32m    696\u001b[0m     should_sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_sync,\n\u001b[0;32m    697\u001b[0m     should_unsync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_unsync,\n\u001b[0;32m    698\u001b[0m ):\n\u001b[1;32m--> 699\u001b[0m     value \u001b[38;5;241m=\u001b[39m _squeeze_if_scalar(\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;66;03m# clone tensor to avoid in-place operations after compute, altering already computed results\u001b[39;00m\n\u001b[0;32m    701\u001b[0m     value \u001b[38;5;241m=\u001b[39m apply_to_collection(value, Tensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mclone())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchmetrics\\classification\\auroc.py:277\u001b[0m, in \u001b[0;36mMulticlassAUROC.compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute metric.\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m state \u001b[38;5;241m=\u001b[39m (dim_zero_cat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreds), dim_zero_cat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthresholds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfmat\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multiclass_auroc_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchmetrics\\functional\\classification\\auroc.py:199\u001b[0m, in \u001b[0;36m_multiclass_auroc_compute\u001b[1;34m(state, num_classes, average, thresholds)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_multiclass_auroc_compute\u001b[39m(\n\u001b[0;32m    194\u001b[0m     state: Union[Tensor, \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor]],\n\u001b[0;32m    195\u001b[0m     num_classes: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    196\u001b[0m     average: Optional[Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    197\u001b[0m     thresholds: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 199\u001b[0m     fpr, tpr, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_multiclass_roc_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reduce_auroc(\n\u001b[0;32m    201\u001b[0m         fpr,\n\u001b[0;32m    202\u001b[0m         tpr,\n\u001b[0;32m    203\u001b[0m         average,\n\u001b[0;32m    204\u001b[0m         weights\u001b[38;5;241m=\u001b[39m_bincount(state[\u001b[38;5;241m1\u001b[39m], minlength\u001b[38;5;241m=\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;28;01mif\u001b[39;00m thresholds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m state[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    205\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchmetrics\\functional\\classification\\roc.py:183\u001b[0m, in \u001b[0;36m_multiclass_roc_compute\u001b[1;34m(state, num_classes, thresholds, average)\u001b[0m\n\u001b[0;32m    181\u001b[0m fpr_list, tpr_list, thres_list \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[1;32m--> 183\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_roc_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     fpr_list\u001b[38;5;241m.\u001b[39mappend(res[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    185\u001b[0m     tpr_list\u001b[38;5;241m.\u001b[39mappend(res[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchmetrics\\functional\\classification\\roc.py:54\u001b[0m, in \u001b[0;36m_binary_roc_compute\u001b[1;34m(state, thresholds, pos_label)\u001b[0m\n\u001b[0;32m     52\u001b[0m     thres \u001b[38;5;241m=\u001b[39m thresholds\u001b[38;5;241m.\u001b[39mflip(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     fps, tps, thres \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Add an extra threshold position to make sure that the curve starts at (0, 0)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     tps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtps\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtps\u001b[38;5;241m.\u001b[39mdevice), tps])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchmetrics\\functional\\classification\\precision_recall_curve.py:73\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(preds, target, sample_weights, pos_label)\u001b[0m\n\u001b[0;32m     71\u001b[0m threshold_idxs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(distinct_value_indices, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], value\u001b[38;5;241m=\u001b[39mtarget\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     72\u001b[0m target \u001b[38;5;241m=\u001b[39m (target \u001b[38;5;241m==\u001b[39m pos_label)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m---> 73\u001b[0m tps \u001b[38;5;241m=\u001b[39m \u001b[43m_cumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[threshold_idxs]\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# express fps as a cumsum to ensure fps is increasing even in\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# the presence of floating point errors\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     fps \u001b[38;5;241m=\u001b[39m _cumsum((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m target) \u001b[38;5;241m*\u001b[39m weight, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[threshold_idxs]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchmetrics\\utilities\\data.py:220\u001b[0m, in \u001b[0;36m_cumsum\u001b[1;34m(x, dim, dtype)\u001b[0m\n\u001b[0;32m    213\u001b[0m     rank_zero_warn(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expect some slowdowns.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    217\u001b[0m         TorchMetricsUserWarning,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39mdim, dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."
     ]
    }
   ],
   "source": [
    "#%% -------- 5. Training Execution --------\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "model = LitMedicalModel(total_steps=EPOCHS * len(train_loader))\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    save_top_k=3,\n",
    "    filename='best-{epoch}-{val_auc:.4f}'\n",
    ")\n",
    "\n",
    "early_stop = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=25,\n",
    "    mode='max',\n",
    "    min_delta=0.005\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=EPOCHS,\n",
    "    precision=PRECISION,\n",
    "    callbacks=[checkpoint, early_stop],\n",
    "    gradient_clip_val=0.5,\n",
    "    accumulate_grad_batches=2,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f32eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 6. Advanced Evaluation --------\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "\n",
    "class MedicalEvaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.tta_transforms = [\n",
    "            lambda x: x,\n",
    "            lambda x: torch.flip(x, [-1]),\n",
    "            lambda x: torch.rot90(x, 1, [-2, -1]),\n",
    "            lambda x: torch.rot90(x, -1, [-2, -1])\n",
    "        ]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad(), autocast():\n",
    "            logits = torch.zeros((x.size(0), NUM_CLASSES), device=x.device)\n",
    "            for transform in self.tta_transforms:\n",
    "                augmented = transform(x)\n",
    "                logits += self.model(augmented)\n",
    "            return logits / len(self.tta_transforms)\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        all_probs = []\n",
    "        \n",
    "        for x, y in loader:\n",
    "            x = x.to(self.model.device)\n",
    "            logits = self.predict(x)\n",
    "            \n",
    "            probs = logits.softmax(1)\n",
    "            preds = probs.argmax(1)\n",
    "            \n",
    "            all_probs.append(probs.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(y)\n",
    "            \n",
    "        return (\n",
    "            torch.cat(all_preds),\n",
    "            torch.cat(all_targets),\n",
    "            torch.cat(all_probs)\n",
    "        )\n",
    "\n",
    "# Load best model\n",
    "best_model = LitMedicalModel.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")\n",
    "evaluator = MedicalEvaluator(best_model)\n",
    "\n",
    "# Test evaluation\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "preds, targets, probs = evaluator.evaluate(test_loader)\n",
    "\n",
    "# Classification report\n",
    "print(\"Medical Diagnostic Report:\")\n",
    "print(classification_report(\n",
    "    targets.numpy(),\n",
    "    preds.numpy(),\n",
    "    target_names=CLASS_NAMES,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(targets.numpy(), preds.numpy())\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title('Medical Diagnostic Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC curves\n",
    "fpr, tpr, _ = roc_curve(targets.numpy(), probs.numpy()[:, 1], pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 7. Visualization Utilities --------\n",
    "def visualize_attention(model, img_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.model.cnn(img_tensor.unsqueeze(0))\n",
    "        features = model.model.adapt(features)\n",
    "        B, C, H, W = features.shape\n",
    "        spatial = features.view(B, C, -1).permute(0, 2, 1)\n",
    "        spatial += model.model.pos_embed\n",
    "        attn_weights = model.model.attention(spatial).squeeze()\n",
    "        \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_tensor.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(attn_weights.view(H, W).cpu().numpy(), cmap='jet')\n",
    "    plt.title(\"Attention Map\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "sample, _ = test_dataset[0]\n",
    "visualize_attention(best_model, sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
